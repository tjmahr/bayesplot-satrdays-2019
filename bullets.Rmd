---
title: "bayesplot bullets"
output: github_document
---

```{r setup, include=FALSE}
fig_width <- 6
fig_height <- 3

knitr::opts_chunk$set(
  echo = TRUE, 
  comment = "#>", 
  collapse = TRUE, 
  dpi = 600, 
  fig.width = fig_width, 
  fig.height = fig_height
)

options(htmltools.dir.version = FALSE)
extrafont::loadfonts(device = "win")

library(tidyverse)
library(rstanarm)
library(tidybayes)
library(bayesplot)

races_raw <- readr::read_tsv("data/scottish-hill-races.txt") %>%
  janitor::clean_names() %>%
  filter(!is.na(mens_time_min)) %>%
  mutate(
    climb_km = climb_m / 1000,
    time_min = if_else(
      mens_time_min < womens_time_min, 
      mens_time_min, 
      womens_time_min, 
      missing = mens_time_min
    )
  ) 

races <- races_raw %>% 
  select(race, distance_km, climb_km, time_min)

ggcaption <- function(x) {
  labs(caption = x)
}

bayesplot_theme_replace(
  plot.caption = element_text(family = "Consolas", hjust = 1, size = rel(.8))
)

m <- stan_glm(
  time_min ~ distance_km,
  data = races,
  prior = normal(0, 1, autoscale = TRUE),
  family = gaussian
)
# library("rstanarm")
# bad_fit <- stan_glm(
#   mpg ~ ., data = mtcars,
#   prior = hs(), iter = 400, adapt_delta = 0.8)
# 


d_fitted_distances <- races %>% 
  tidyr::expand(
    distance_km = seq(min(distance_km), max(distance_km), length.out = 100)
  ) %>% 
  tidybayes::add_fitted_draws(m, n = 20)
```


# bayesplot

Diagnostic plots for Bayesian models

```{r}
library(bayesplot)
```


# Why are Bayesian models different?

- Classical regression: line of best fit (maximum likelihood)

- Bayesian regression: all plausible lines given data and model (posterior distribution)

Flexible. Generative. Include prior information. Quantify uncertainty about any
statistic or prediction from the model.

# The problem

- That distribution is hard to compute so we approximate it by sampling from it.
- The model is a collection of parameter draws from the distribution. 


# Toy model

Scottish Hill races. Try to predict race time from race distance and hill height.

```{r}
races
```


```{r, eval = TRUE, results='hide'}
m1 <- stan_glm(
  time_min ~ distance_km,
  data = races,
  family = gaussian,
  prior = normal(0, 1, autoscale = TRUE)
)
m2 <- stan_glm(
  time_min ~ distance_km * climb_km,
  data = races,
  family = gaussian,
  prior = normal(0, 1, autoscale = TRUE)
)
```

# The model is a distribution


```{r, fig.show="hide"}
m1_draws <- as.matrix(m1) 
mcmc_hist(m1_draws, pars = c("(Intercept)", "distance_km"))
```
```{r, echo = FALSE}
last_plot() +
  ggcaption("mcmc_hist(draws, pars)")
```

```{r, fig.show='hide'}
m1_draws <- as.matrix(m1) 
mcmc_hist(m1_draws, pars = c("(Intercept)", "distance_km"))
```
```{r, echo = FALSE, message = FALSE}
last_plot() +
  ggcaption("mcmc_hist(draws, pars)")
```

```{r, fig.show='hide'}
mcmc_intervals(m1_draws)
```
```{r, echo = FALSE, message = FALSE}
last_plot() +
  ggcaption("mcmc_intervals(draws)")
```

```{r, fig.show='hide'}
mcmc_areas(m1_draws, area_method = "equal height")
```
```{r, echo = FALSE}
last_plot() +
  ggcaption("mcmc_areas(draws, area_method)")
```


```{r}
color_scheme_set("orange")
multilevel <- shinystan::eight_schools@posterior_sample
mcmc_areas_ridges(multilevel, prob = .8)
```
```{r, echo = FALSE}
last_plot() +
  ggcaption("mcmc_areas_ridges(draws, prob)")
color_scheme_set("blue")
```

## Joint distributions

Those other views are looking at the side of hill. We can look from above too.


```{r, fig.show='hide'}
mcmc_scatter(m1_draws, c("(Intercept)", "distance_km"), alpha = 0.15) + 
  stat_density_2d(color = "black", size = .25)
```
```{r, echo = FALSE, fig.width=fig_width - 1}
last_plot() +
  ggcaption("mcmc_scatter(draws, pars, alpha) + stat_density_2d(color, size)")
```


```{r, fig.show='hide'}
mcmc_hex(m1_draws, c("(Intercept)", "distance_km"))
```
```{r, echo = FALSE, fig.width=5}
last_plot() +
  ggcaption("mcmc_hex(draws, pars)")
```





# Generative models

Bayesian models are generative. You describe a data-generating process and it
provides parameters for the process that compatible with the data.

Posterior predictive checks: If I make the model simulate new data, does it look
like the original data?

# Posterior Predictive Boxplots

Do the simulations have similar quantiles?

```{r, fig.show='hide'}
yrep1 <- posterior_predict(m1, draws = 40)
ppc_boxplot(y = races$time_min, yrep = yrep1[1:6, ])
```
```{r, echo = FALSE}
last_plot() +
  ggcaption("ppc_boxplot(y, yrep)")
```

# Posterior Predictive Density

```{r, fig.show='hide'}
ppc_dens_overlay(y = races$time_min, yrep = yrep1) + 
  ggtitle("time_min ~ distance_km")
```
```{r, echo = FALSE}
last_plot() + 
  ggcaption("ppc_dens_overlay(y, yrep)") 
```

```{r, fig.show='hide'}
yrep2 <- posterior_predict(m2, draws = 40)
color_scheme_set("purple")
ppc_dens_overlay(y = races$time_min, yrep = yrep2) + 
  ggtitle("time_min ~ distance_km * climb_km")
```
```{r, echo = FALSE}
last_plot() + 
  ggcaption("ppc_dens_overlay(y, yrep)") 
color_scheme_set("blue")
```


# Pointwise Posterior Predictions

How well are individual data points predicted?

```{r, fig.show='hide'}
yrep1 <- posterior_predict(m1, draws = 1000)
ppc_intervals(races$time_min, yrep1, x = races$distance_km, prob_outer = .95)
```
```{r, echo = FALSE}
last_plot() +
  ggcaption("ppc_intervals(y, yrep, x, prob_outer)")
```

# Pointwise Posterior Predictive Error

Error of individual predictions?

```{r, fig.show='hide'}
ppc_error_scatter_avg_vs_x(m1$y, yrep1, x = races$climb_km) + 
  stat_smooth(color = "grey30")
```
```{r, echo = FALSE}
last_plot() +
  ggcaption("ppc_error_scatter_avg_vs_x(y, yrep, x) + stat_smooth()")
```

```{r}
ppc_stat(m1$y, yrep1, stat = "mean")

```


# trace plots

Bayesian models are estimated by Markov Chain Monte Carlo. We run chains


```{r}

m <- stan_glm(
  mens_time_min ~ distance_km,
  data = races_raw,
  prior = normal(0, 1),
  family = gaussian
)


# Change y to mimic a chain being stuck in a different mode
m1 <- stan_glm(
  womens_time_min ~ distance_km,
  data = races_raw,
  family = gaussian
)

chains <- as.array(m)
chains[1:400, 2, ] <- as.array(m1)[1:400, 2, ]

draws <- as.matrix(m)
mcmc_trace(chains)
mcmc_areas(chains)
```

Fuzzy caterpillar: Good mixing of chains

```{r}
mcmc_trace(as.array(m), "distance_km") + facet_wrap("chain")
```

Bad mixing of chains

```{r}
mcmc_trace(chains, "distance_km")
```

```{r}

# as.array(chains)[, 3, ] %>% as.data.frame() %>% 
#   ggplot() + 
#   aes(x = `(Intercept)`, y = distance_km) + 
#   geom_path(alpha = .2)
# as.data.frame()
```
